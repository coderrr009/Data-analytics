{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of classification?**"
      ],
      "metadata": {
        "id": "EkDcxj-ecfPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans1. A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, it acts like a flowchart where each internal node represents a decision based on a feature, each branch represents an outcome of the decision, and each leaf node represents a class label.\n",
        "\n",
        "Working:\n",
        "\n",
        "The algorithm begins at the root node and splits the dataset based on feature values that best separate the classes.\n",
        "\n",
        "This splitting continues recursively until a stopping condition is met (like max depth or pure nodes).\n",
        "\n",
        "The path from the root to a leaf represents a classification rule."
      ],
      "metadata": {
        "id": "nB3nuF7Ecg4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**"
      ],
      "metadata": {
        "id": "JnQ-GK4ycoBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 2. Gini Impurity measures the probability of misclassifying a randomly chosen element if it were labeled randomly according to the class distribution.\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1− i=1∑npi2\n",
        "​\n",
        "\n",
        "Entropy measures the amount of disorder or randomness in the data.\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "𝑝\n",
        "𝑖\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        "\n",
        "Impact on Splits:\n",
        "\n",
        "Both are used to evaluate how \"pure\" a split is.\n",
        "\n",
        "The split that results in the lowest Gini or Entropy is chosen.\n",
        "\n",
        "Gini is faster to compute, whereas Entropy gives more information-theoretic insight.\n",
        "\n"
      ],
      "metadata": {
        "id": "auMC8bvTcwqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**"
      ],
      "metadata": {
        "id": "4FVlL21jdBpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 3. Pre-Pruning: Stops the tree from growing beyond a certain depth or if a split doesn’t improve the metric significantly.\n",
        "\n",
        "Advantage: Prevents overfitting and saves computation.\n",
        "\n",
        "Post-Pruning: Builds a full tree and then removes branches that have little impact.\n",
        "\n",
        "Advantage: Allows the model to explore complex patterns first and then simplifies."
      ],
      "metadata": {
        "id": "QUf2SaZ-dH_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**"
      ],
      "metadata": {
        "id": "R2ejAzvjdMM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 4. Information Gain is the reduction in entropy (or impurity) after a dataset is split on an attribute. It is calculated as:\n",
        "\n",
        "𝐼\n",
        "𝑛\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑚\n",
        "𝑎\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "\n",
        "𝐺\n",
        "𝑎\n",
        "𝑖\n",
        "𝑛\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "−\n",
        "∑\n",
        "(\n",
        "∣\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "∣\n",
        "∣\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "∣\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        ")\n",
        ")\n",
        "Information Gain=Entropy(parent)−∑(\n",
        "∣parent∣\n",
        "∣child∣\n",
        "​\n",
        " ×Entropy(child))\n",
        "Importance:\n",
        "\n",
        "Higher information gain indicates a better split.\n",
        "\n",
        "Helps in selecting the most informative feature at each node.\n",
        "\n"
      ],
      "metadata": {
        "id": "4X9yB5LMdPJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**"
      ],
      "metadata": {
        "id": "NdWHZZdte8ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 5. Applications:\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Credit risk assessment\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Marketing segmentation\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to interpret and visualize\n",
        "\n",
        "Handles both categorical and numerical data\n",
        "\n",
        "Requires little data preparation\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Unstable to small data changes\n",
        "\n",
        "Can be biased with imbalanced data"
      ],
      "metadata": {
        "id": "dIN8AkqEfBKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Python Program (Iris Dataset, Gini Criterion)**"
      ],
      "metadata": {
        "id": "VjfnzljLfG63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3zO19_xfLPf",
        "outputId": "d9da21ff-6b07-4b83-bbf8-d3c07aa86643"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.01911002 0.         0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Question 7: Compare Decision Trees with max_depth=3 and fully-grown tree**"
      ],
      "metadata": {
        "id": "1jRQdT_lfR9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully grown tree\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "# Limited depth tree\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "acc_limited = accuracy_score(y_test, clf_limited.predict(X_test))\n",
        "\n",
        "print(\"Full Tree Accuracy:\", acc_full)\n",
        "print(\"Depth=3 Tree Accuracy:\", acc_limited)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCYu6litfa3R",
        "outputId": "68d775fc-29e6-48dd-afdf-76ae5081b177"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.0\n",
            "Depth=3 Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Decision Tree Regressor on Boston Housing**"
      ],
      "metadata": {
        "id": "sbjjsZCEfczw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "data = load_boston()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "predictions = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)\n"
      ],
      "metadata": {
        "id": "Vu6yryPofgb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: GridSearchCV Tuning on Iris Dataset**"
      ],
      "metadata": {
        "id": "IK2VY35ifm62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), params, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), params, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZbUXo3kfvXG",
        "outputId": "df95532c-ebf4-483c-afb4-b8e6dd3f6e67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'min_samples_split': 4}\n",
            "Best Accuracy: 0.9238095238095237\n",
            "Best Parameters: {'max_depth': 2, 'min_samples_split': 4}\n",
            "Best Accuracy: 0.9238095238095237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Step-by-step for Disease Prediction Model**"
      ],
      "metadata": {
        "id": "DdVmAZ-Sfzp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 10. Handle Missing Values:\n",
        "\n",
        "Use imputation: SimpleImputer(strategy='mean' or 'most_frequent')\n",
        "\n",
        "Drop rows/columns with too many missing values\n",
        "\n",
        "Encode Categorical Features:\n",
        "\n",
        "Use OneHotEncoder or LabelEncoder\n",
        "\n",
        "Train Decision Tree:\n",
        "\n",
        "Split into train-test sets\n",
        "\n",
        "Use DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "Tune Hyperparameters:\n",
        "\n",
        "Use GridSearchCV with parameters like max_depth, min_samples_split\n",
        "\n",
        "Evaluate Performance:\n",
        "\n",
        "Accuracy, Precision, Recall, F1-score\n",
        "\n",
        "Confusion Matrix and ROC Curve\n",
        "\n",
        "Business Value:\n",
        "\n",
        "Early detection leads to timely treatment\n",
        "\n",
        "Reduces operational costs by automating diagnosis\n",
        "\n",
        "Enhances patient care and prioritization\n",
        "\n"
      ],
      "metadata": {
        "id": "4OPmPu_9f3-1"
      }
    }
  ]
}