{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "\n",
        "Answer:\n",
        "KNN is a supervised learning algorithm that makes predictions based on the similarity between data points.\n",
        "\n",
        "In classification, it assigns a class label to a new point by looking at the majority class among its k nearest neighbors.\n",
        "\n",
        "In regression, it predicts a continuous value by taking the average (or weighted average) of the target values of its k nearest neighbors.\n",
        "Distance metrics like Euclidean, Manhattan, or Minkowski are commonly used to find neighbors."
      ],
      "metadata": {
        "id": "rJC4p2cRPITr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "\n",
        "Answer:\n",
        "The Curse of Dimensionality refers to the problems that arise when data has too many features (high dimensions).\n",
        "\n",
        "In high dimensions, distances between points become less meaningful — all points tend to appear equally far apart.\n",
        "\n",
        "For KNN, this means neighbors are not truly “close,” reducing classification/regression accuracy.\n",
        "\n",
        "It also increases computation cost."
      ],
      "metadata": {
        "id": "87L5ttjBPR36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. **What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "PCA is a dimensionality reduction technique that transforms data into a new coordinate system, where the new features (principal components) capture the maximum variance in the data.\n",
        "\n",
        "It is different from feature selection because PCA creates new features (linear combinations of original ones), whereas feature selection simply chooses a subset of existing features."
      ],
      "metadata": {
        "id": "__4KMWoVdDHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "Answer:\n",
        "\n",
        "Eigenvectors define the direction of the new feature axes (principal components).\n",
        "\n",
        "Eigenvalues represent the amount of variance captured by each eigenvector.\n",
        "\n",
        "Importance: By ranking eigenvalues, we know which components capture the most information, allowing us to reduce dimensionality effectively."
      ],
      "metadata": {
        "id": "UoNqz_26dTVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "PCA reduces the dimensionality, removing noise and redundant features.\n",
        "\n",
        "KNN then operates on this reduced dataset, improving efficiency and accuracy.\n",
        "\n",
        "Together, they handle high-dimensional data better and avoid overfitting"
      ],
      "metadata": {
        "id": "qMhYB2aVdh1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare accuracy.**"
      ],
      "metadata": {
        "id": "J8XX0wRhdneE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "acc_without = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn.predict(X_test_scaled)\n",
        "acc_with = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_without)\n",
        "print(\"Accuracy with scaling:\", acc_with)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTrtndpLdrB7",
        "outputId": "26d902a6-1eb6-4798-c1a3-0984d42dd5f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Train KNN on PCA-transformed dataset (top 2 components). Compare accuracy.**"
      ],
      "metadata": {
        "id": "FlFXOIWAeAOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_2 = PCA(n_components=2)\n",
        "X_train_pca = pca_2.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_2.transform(X_test_scaled)\n",
        "\n",
        "knn.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy with original data:\", acc_with)\n",
        "print(\"Accuracy with PCA (2 components):\", acc_pca)\n"
      ],
      "metadata": {
        "id": "v7BDFoGjeHBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Train KNN with different distance metrics (euclidean, manhattan). Compare results.**"
      ],
      "metadata": {
        "id": "2IY1qrGueN_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    print(f\"Accuracy with {metric} distance:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsjgnMZVeluD",
        "outputId": "f4ab55cb-3a91-4794-8f06-2f4be29cf020"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with euclidean distance: 0.9444444444444444\n",
            "Accuracy with manhattan distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. High-dimensional gene dataset pipeline explanation**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Use PCA to reduce dimensionality → Keeps only the components that explain most of the variance, removing noise.\n",
        "\n",
        "Decide components to keep → Use explained variance ratio or cumulative variance plot (e.g., retain 95% variance).\n",
        "\n",
        "Use KNN for classification → After PCA transformation, apply KNN with an appropriate distance metric.\n",
        "\n",
        "Evaluate the model → Use cross-validation and metrics like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "Justification:\n",
        "\n",
        "PCA reduces risk of overfitting in high dimensions.\n",
        "\n",
        "KNN is simple and interpretable for stakeholders.\n",
        "\n",
        "Pipeline is computationally efficient and robust for biomedical datasets with few samples and many features."
      ],
      "metadata": {
        "id": "yFCW5-ejen4Z"
      }
    }
  ]
}